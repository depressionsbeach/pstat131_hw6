---
title: "Homework 6"
author: "Baiming WANG PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

```{r, echo=FALSE, eval=FALSE}
#install.packages('rpart.plot')
#install.packages('vip')
#install.packages('randomForest')
#install.packages('xgboost')
#install.packages('ranger')
```


```{r, echo=FALSE}
library(janitor)
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(corrplot)
library(rpart.plot)
library(vip)
library(randomForest)
library(ranger)
library(xgboost)
tidymodels_prefer()
```

### Exercise 1

Read in the data and set things up as in Homework 5:

- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors

```{r}
pokemon <- read.csv('data/pokemon.csv') %>% 
  clean_names() %>% 
  filter(type_1 %in% c('Bug', 'Fire', 'Grass', 'Normal','Water', 'Psychic'))
pokemon$type_1 <- factor(pokemon$type_1)
pokemon$legendary <- factor(pokemon$legendary)
head(pokemon)
```


Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

```{r}
set.seed(1729)
pokemon_split <- initial_split(data = pokemon, prop = 3/4, strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

```{r}
pokemon_folds <- pokemon_train %>%
  vfold_cv(v = 5, strata = type_1)
```

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r}
pokemon_recipe <- pokemon_train %>%
  recipe(type_1 ~ legendary + generation + sp_atk + sp_def + attack + defense + speed + hp) %>%
  step_normalize(all_numeric()) %>%
  step_dummy(c(legendary, generation))
```


### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

```{r}
pokemon_train %>%
  select(c(sp_atk, sp_def, attack, defense, speed, hp)) %>%
  na.omit() %>%
  cor() %>%
  corrplot(type = 'lower', method = "number")
```


What relationships, if any, do you notice? Do these relationships make sense to you?
<br />
**I noticed that all of the predictors are positively correlated with each other. This might make sense (?) From a commercial perspective, you need some pokemons to be better than others in almost all areas to justify the difference in price and rarity. **


### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

```{r}
tree_spec <- decision_tree(cost_complexity = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_wf <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(pokemon_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
```

```{r, eval=FALSE}
tune_res <- tune_grid(
  tree_wf, 
  resamples = pokemon_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
```

```{r, echo=FALSE}
#save(tune_res, file = 'tune_res.rda')
load(file='tune_res.rda')
```

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r}
autoplot(tune_res)
```
<br />
**It generally performs better with a smaller complexity penalty, but the performance peaks with a relatively large penalty.** 

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
tree_final <- tune_res %>%
  select_best(metric = 'roc_auc') %>%
  finalize_workflow(x = tree_wf) 

tree_fit_folds <- tree_final %>%
  fit_resamples(resamples = pokemon_folds)

collect_metrics(tree_fit_folds, summarize = FALSE)[c(2,4,6,8,10), c(1,2,4)]
```
### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
tree_final %>%
  fit(data = pokemon_train) %>%
  extract_fit_engine() %>%
  rpart.plot()
```


### Exercise 6

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

```{r}
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(pokemon_recipe)
```
<br />
**'mtry' represents the number of randomly sampled predictors at each split of the tree model. 'trees' represent the total number of trees. 'min_n' is the minimum number of data points in a node for it to be further split.**

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r}
rf_grid <- grid_regular(mtry(range = c(1, 8)), trees(range = c(200, 2000)), min_n(), levels = 8)
```
<br />
**When mtry=8, we all essentially selecting all the predictors in the formula. Therefore, mtry cannot be greater than 8.**

### Exercise 7

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?
```{r, eval=FALSE}
rf_tune_res <- tune_grid(
  rf_wf, 
  resamples = pokemon_folds, 
  grid = rf_grid, 
  metrics = metric_set(roc_auc)
)
```

```{r, echo=FALSE}
#save(rf_tune_res, file = 'rf_tune_res.rda')
load(file = 'rf_tune_res.rda')
```

```{r}
autoplot(rf_tune_res)
```
```{r, echo=FALSE, eval=FALSE}
rf_tune_res %>%
  select_best(metric = 'roc_auc')
```
<br />
**It seems that a combination of mtry=2, trees=457, and min_n=7 yields the best performance.**

### Exercise 8

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
rf_final <- rf_tune_res %>%
  select_best(metric = 'roc_auc') %>%
  finalize_workflow(x = rf_wf) 

rf_fit_folds <- rf_final %>%
  fit_resamples(resamples = pokemon_folds)

collect_metrics(rf_fit_folds, summarize = FALSE)[c(2,4,6,8,10), c(1,2,4)]
```

### Exercise 9

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

```{r}
rf_fit <- rf_final %>%
  fit(data = pokemon_train) 

rf_fit %>%
  pull_workflow_fit() %>%
  vip()
```

Which variables were most useful? Which were least useful? Are these results what you expected, or not?
<br />
**sp_atk is the most useful variable, and legendary is the least useful variable. Expected or unexpected? I can't call it either way. I didn't really have an expectation as I know nothing about how Pokemon works...**

### Exercise 10

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

```{r}
boost_spec <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_grid <- grid_regular(trees(c(10, 2000)), levels = 10)

boost_wf <- workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(pokemon_recipe)
```

```{r, eval=FALSE}
boost_tune_res <- tune_grid(
  boost_wf,
  resamples = pokemon_folds,
  grid = boost_grid,
  metrics = metric_set(roc_auc)
)
```

```{r, echo=FALSE}
#save(boost_tune_res, file='boost_tune_res.rda')
load(file='boost_tune_res.rda')
```

```{r}
autoplot(boost_tune_res)
```

What do you observe?
<br />
**The model's performance initially drastically increases but then steadily decreases afterwards. The peak is at trees=231. **

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
boost_final <- boost_tune_res %>%
  select_best(metric = 'roc_auc') %>%
  finalize_workflow(x = boost_wf) 

boost_fit_folds <- boost_final %>%
  fit_resamples(resamples = pokemon_folds)

collect_metrics(boost_fit_folds, summarize = FALSE)[c(2,4,6,8,10), c(1,2,4)]
```

### Exercise 11

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set.

```{r}
auc_mean <-c(collect_metrics(tree_fit_folds)[[2, 3]],
  collect_metrics(rf_fit_folds)[[2, 3]], 
  collect_metrics(boost_fit_folds)[[2, 3]])

model <- c("Tree", "Random Forest", "Boost")
summary_df <- data.frame(mean_roc_auc = auc_mean, model = model)
summary_df
```
<br />
**Random Forest is the best model among the three.**

```{r}
pokemon_final <- rf_final
pokemon_fit <- fit(pokemon_final, data=pokemon_train)
pokemon_test_w_pred<-
  augment(pokemon_fit, new_data = pokemon_test)
head(pokemon_test_w_pred)
```

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.
<br />
**The AUC value is 0.736.**
```{r,echo=FALSE}
pokemon_test_w_pred %>%
  roc_auc(truth=type_1, .pred_Bug:.pred_Water)
```
<br />
**ROC curves:**
```{r, echo=FALSE}
pokemon_test_w_pred %>%
  roc_curve(truth=type_1, .pred_Bug:.pred_Water) %>%
  autoplot()
```
<br />
**Heat Map: **
```{r,echo=FALSE}
pokemon_test_w_pred%>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = 'heatmap')
```

Which classes was your model most accurate at predicting? Which was it worst at?
<br />
**The model is best at predicting Normal class, and worst at predicting Grass class.**

## For 231 Students

### Exercise 12

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?